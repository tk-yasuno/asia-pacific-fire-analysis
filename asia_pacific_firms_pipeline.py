#!/usr/bin/env python3
"""
ã‚¢ã‚¸ã‚¢å¤ªå¹³æ´‹åœ°åŸŸ NASA FIRMS æ£®æ—ç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
æ—¥æœ¬ã‚’ä¸­å¿ƒã¨ã™ã‚‹ã‚¢ã‚¸ã‚¢å¤ªå¹³æ´‹ã‚¨ãƒªã‚¢ã®ç«ç½ãƒ‡ãƒ¼ã‚¿        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            raise
    
    def run_full_pipeline(self, sample_size: int = 1000):ã‚©ãƒ«ãƒ€ã«ä¿å­˜

ã‚¨ãƒªã‚¢ç¯„å›²:
- ç·¯åº¦: 10Â°N ~ 50Â°N (æ±å—ã‚¢ã‚¸ã‚¢ã€œãƒ­ã‚·ã‚¢æ¥µæ±)
- çµŒåº¦: 100Â°E ~ 180Â°E (ã‚¤ãƒ³ãƒ‰ã€œå¤ªå¹³æ´‹)
- å¯¾è±¡å›½: æ—¥æœ¬ã€éŸ“å›½ã€ä¸­å›½ã€å°æ¹¾ã€ãƒ•ã‚£ãƒªãƒ”ãƒ³ã€ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢ã€ãƒãƒ¬ãƒ¼ã‚·ã‚¢ã€ã‚¿ã‚¤ã€ãƒ™ãƒˆãƒŠãƒ ç­‰
"""

import os
import sys
import json
import logging
import time
import time
from datetime import datetime
from typing import Dict, List, Optional

# ãƒ‘ã‚¹è¨­å®š
scripts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'scripts')
sys.path.append(scripts_dir)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scripts.data_collector import DataCollector
from scripts.model_loader import ModelLoader
from scripts.embedding_generator import EmbeddingGenerator
from adaptive_clustering_selector import AdaptiveClusteringSelector
from scripts.visualization import VisualizationManager
from cluster_feature_analyzer import ClusterFeatureAnalyzer
from fire_analysis_report_generator import FireAnalysisReportGenerator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)


def _time_step(step_name):
    """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚é–“æ¸¬å®šç”¨ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        def wrapper(self, *args, **kwargs):
            start_time = time.time()
            logger.info(f"=== Starting: {step_name} ===")
            try:
                result = func(self, *args, **kwargs)
                end_time = time.time()
                logger.info(f"=== Completed: {step_name} ({end_time - start_time:.2f}s) ===")
                return result
            except Exception as e:
                end_time = time.time()
                logger.error(f"=== Failed: {step_name} ({end_time - start_time:.2f}s) - {e} ===")
                raise
        return wrapper
    return decorator


class AsiaPacificFirmsAnalyzer:
    """ã‚¢ã‚¸ã‚¢å¤ªå¹³æ´‹åœ°åŸŸNASA FIRMSæ£®æ—ç«ç½åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path: str = "config_asia_pacific_firms.json"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        timestamp = datetime.now().strftime(self.config['output']['timestamp_format'])
        self.output_dir = f"data_firms_{timestamp}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        logger.info(f"Output directory: {self.output_dir}")
        
        # å‡¦ç†æ™‚é–“è¨˜éŒ²
        self.step_times = {}
        
    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"Configuration loaded: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            raise
    
    def _time_step(self, step_name: str):
        """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚é–“æ¸¬å®šãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                elapsed = time.time() - start_time
                self.step_times[step_name] = elapsed
                logger.info(f"Step '{step_name}' completed in {elapsed:.2f}s")
                return result
            return wrapper
        return decorator
    
    def run_pipeline(self) -> str:
        """
        ã‚¢ã‚¸ã‚¢å¤ªå¹³æ´‹åœ°åŸŸç«ç½åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        
        Returns:
            çµæœãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        logger.info("ğŸŒ Starting Asia-Pacific FIRMS Fire Analysis Pipeline")
        pipeline_start = time.time()
        
        try:
            # Step 1: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            logger.info("=== Initializing Pipeline Components ===")
            model_loader, embedding_generator, clustering_selector, data_collector, viz_manager, cluster_analyzer, report_generator = self._initialize_components()
            
            # Step 2: NASA FIRMSãƒ‡ãƒ¼ã‚¿åé›†
            logger.info("=== Collecting NASA FIRMS Data (Asia-Pacific Region) ===")
            nasa_data = self._collect_nasa_firms_data(data_collector)
            
            if len(nasa_data) == 0:
                logger.error("No NASA FIRMS data collected")
                return None
            
            # Step 3: åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            logger.info("=== Generating Text Embeddings ===")
            embeddings, scores = self._generate_embeddings(embedding_generator, nasa_data)
            
            # Step 4: é©å¿œã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            logger.info("=== Performing Adaptive Clustering ===")
            clustering_result = self._perform_clustering(clustering_selector, embeddings)
            
            # Step 5: å¯è¦–åŒ–ä½œæˆ
            logger.info("=== Creating Comprehensive Visualizations ===")
            self._create_visualizations(viz_manager, embeddings, clustering_result, nasa_data)
            
            # Step 6: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å¾´åˆ†æ
            logger.info("=== Performing Cluster Feature Analysis ===")
            feature_analysis = self._perform_cluster_feature_analysis(cluster_analyzer, nasa_data, clustering_result, embeddings)
            
            # Step 7: çµæœä¿å­˜
            logger.info("=== Saving Final Results ===")
            result_path = self._save_results(clustering_result, nasa_data, feature_analysis)
            
            # Step 8: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            logger.info("=== Generating Comprehensive Analysis Report ===")
            report_path = self._generate_comprehensive_report(report_generator, clustering_result, feature_analysis, nasa_data)
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†
            total_time = time.time() - pipeline_start
            logger.info("ğŸ‰ Asia-Pacific Fire Analysis Pipeline completed successfully!")
            logger.info(f"Processing time: {total_time:.2f}s for {len(nasa_data)} samples")
            
            self._print_summary(clustering_result, nasa_data, total_time)
            
            return result_path
            
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            raise
    
    def _initialize_components(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼
        model_loader = ModelLoader(
            model_name=self.config['embedding']['model_name'],
            device=self.config['embedding']['device']
        )
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå™¨
        embedding_generator = EmbeddingGenerator(
            model=model_loader.load_model(),
            batch_size=self.config['embedding']['batch_size']
        )
        
        # é©å¿œã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        clustering_selector = AdaptiveClusteringSelector(
            output_dir=self.output_dir
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
        data_collector = DataCollector()
        
        # å¯è¦–åŒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        viz_manager = VisualizationManager(
            output_dir=self.output_dir,
            figsize=tuple(self.config['visualization']['figsize'])
        )
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å¾´åˆ†æå™¨
        cluster_analyzer = ClusterFeatureAnalyzer(output_dir=self.output_dir)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨
        report_generator = FireAnalysisReportGenerator(output_dir=self.output_dir)
        
        logger.info("All components initialized successfully")
        return model_loader, embedding_generator, clustering_selector, data_collector, viz_manager, cluster_analyzer, report_generator
    
    def _collect_nasa_firms_data(self, data_collector: DataCollector):
        """NASA FIRMSãƒ‡ãƒ¼ã‚¿åé›†"""
        nasa_config = self.config['nasa_firms']
        
        # ã‚¢ã‚¸ã‚¢å¤ªå¹³æ´‹åœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿å–å¾—
        df = data_collector.collect_nasa_firms_data(
            area_params=nasa_config['area_params'],
            days_back=nasa_config['days_back'],
            satellite=nasa_config['satellite']
        )
        
        if len(df) == 0:
            logger.warning("No fire data found in Asia-Pacific region")
            return []
        
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç† - é«˜ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        max_samples = self.config['processing']['max_samples']
        initial_count = len(df)
        
        if max_samples and len(df) > max_samples:
            logger.warning(f"Data exceeds max_samples limit ({len(df)} > {max_samples})")
            logger.warning(f"Truncating to first {max_samples} samples for system stability")
            df = df.iloc[:max_samples]  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã¯ãªãå…ˆé ­ã‹ã‚‰åˆ‡ã‚Šå–ã‚Š
        else:
            logger.info(f"Processing all {len(df)} high-confidence fire detections (no sampling)")
        
        logger.info(f"Final dataset: {len(df)} NASA FIRMS records for comprehensive analysis")
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        data_path = os.path.join(self.output_dir, "nasa_firms_data.csv")
        df.to_csv(data_path, index=False)
        logger.info(f"Data saved: {data_path}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆç”¨ï¼‰
        texts = []
        for _, row in df.iterrows():
            text = f"Fire detection: Lat={row['latitude']:.3f}, Lon={row['longitude']:.3f}, " \
                   f"Brightness={row['brightness']:.1f}, Confidence={row['confidence']:.1f}%, " \
                   f"Date={row['acq_date']} {row['acq_time']}, Satellite={row['satellite']}"
            texts.append(text)
        
        return texts
    
    def _generate_embeddings(self, embedding_generator: EmbeddingGenerator, texts: List[str]):
        """åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        embeddings, scores = embedding_generator.generate_embeddings_batch(texts)
        
        logger.info(f"Generated {len(embeddings)} embeddings (dim: {embeddings.shape[1]}) at {len(texts)/(self.step_times.get('Embedding Generation', 1)):.2f} texts/sec")
        
        # åŸ‹ã‚è¾¼ã¿ä¿å­˜
        import numpy as np
        embeddings_path = os.path.join(self.output_dir, "embeddings.npy")
        np.save(embeddings_path, embeddings.cpu().numpy())
        logger.info(f"Embeddings saved: {embeddings_path}")
        
        return embeddings, scores
    
    def _perform_clustering(self, clustering_selector: AdaptiveClusteringSelector, embeddings):
        """é©å¿œã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        hdbscan_params = self.config['adaptive_clustering']['hdbscan_params']
        kmeans_params = self.config['adaptive_clustering']['kmeans_params']
        
        logger.info(f"Adaptive parameters: HDBSCAN min_cluster_size={hdbscan_params['min_cluster_size']}, k-means n_clusters={kmeans_params['n_clusters']}")
        
        # é©å¿œã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        clustering_result, result_info = clustering_selector.select_best_clustering(embeddings)
        
        # result_infoã®å®‰å…¨ãªå‡¦ç†
        method_info = result_info.get('selection_reason', 'unknown selection reason')
        selected_method = clustering_result.method if hasattr(clustering_result, 'method') else 'unknown method'
        
        logger.info(f"Selected method: {selected_method} ({method_info})")
        
        return clustering_result
    
    def _create_visualizations(self, viz_manager: VisualizationManager, embeddings, clustering_result, nasa_data):
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ä½œæˆ"""
        import numpy as np
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        labels = clustering_result.labels
        embeddings_np = embeddings.cpu().numpy() if hasattr(embeddings, 'cpu') else embeddings
        
        # ãƒ€ãƒŸãƒ¼ã‚¹ã‚³ã‚¢ä½œæˆï¼ˆå®Ÿéš›ã«ã¯embedding_generatorã‹ã‚‰å–å¾—ï¼‰
        scores = np.random.rand(len(labels))
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨IDã®æº–å‚™
        texts = [f"Fire detected at index {i}: {text}" for i, text in enumerate(nasa_data)]
        ids = [f"fire_{i:04d}" for i in range(len(texts))]
        
        # åˆ†æçµæœã®æº–å‚™
        unique_labels = np.unique(labels[labels >= 0])
        cluster_stats = {}
        for label in unique_labels:
            mask = labels == label
            cluster_stats[int(label)] = {
                'n_samples': int(np.sum(mask)),
                'score_mean': float(scores[mask].mean()),
                'score_std': float(scores[mask].std())
            }
        
        analysis = {
            'cluster_stats': cluster_stats,
            'quality_score': clustering_result.metrics.quality_score,
            'n_clusters': len(unique_labels),
            'noise_ratio': (labels == -1).sum() / len(labels),
            'method': clustering_result.method
        }
        
        # å¯è¦–åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        try:
            plot_files = viz_manager.run_visualization_pipeline(
                features=embeddings_np,
                labels=labels,
                scores=scores,
                texts=texts,
                ids=ids,
                analysis=analysis
            )
            
            logger.info(f"âœ… Generated {len(plot_files)} visualization files")
            for name, path in plot_files.items():
                logger.info(f"  ğŸ“Š {name}: {path}")
                
        except Exception as e:
            logger.error(f"Visualization failed: {e}")
            # å¯è¦–åŒ–ãŒå¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œ
    
    def _perform_cluster_feature_analysis(self, cluster_analyzer: ClusterFeatureAnalyzer, nasa_data: List, clustering_result, embeddings):
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å¾´åˆ†æå®Ÿè¡Œ"""
        import pandas as pd
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŸºæœ¬çš„ãªåœ°ç†çš„æƒ…å ±ã‚’æŠ½å‡º
        # nasa_dataã¯ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆãªã®ã§ã€ä¿å­˜ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        data_path = os.path.join(self.output_dir, "nasa_firms_data.csv")
        nasa_df = pd.read_csv(data_path)
        
        # å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        required_columns = ['latitude', 'longitude', 'brightness', 'confidence', 'acq_date', 'acq_time']
        for col in required_columns:
            if col not in nasa_df.columns:
                logger.warning(f"Missing column: {col}, setting default values")
                nasa_df[col] = 0 if col in ['latitude', 'longitude', 'brightness', 'confidence'] else ''
        
        # FRPåˆ—ã‚’è¿½åŠ ï¼ˆNASA FIRMSãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ï¼‰
        if 'frp' not in nasa_df.columns:
            nasa_df['frp'] = 0
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åŸ‹ã‚è¾¼ã¿æ•°ã«åˆã‚ã›ã‚‹
        if len(nasa_df) > len(clustering_result.labels):
            nasa_df = nasa_df.iloc[:len(clustering_result.labels)]
        elif len(nasa_df) < len(clustering_result.labels):
            # ä¸è¶³åˆ†ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padding_rows = len(clustering_result.labels) - len(nasa_df)
            padding_df = pd.DataFrame({
                'latitude': [0] * padding_rows,
                'longitude': [0] * padding_rows,
                'brightness': [0] * padding_rows,
                'confidence': [0] * padding_rows,
                'acq_date': [''] * padding_rows,
                'acq_time': [''] * padding_rows,
                'frp': [0] * padding_rows
            })
            nasa_df = pd.concat([nasa_df, padding_df], ignore_index=True)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å¾´åˆ†æå®Ÿè¡Œ
        feature_analysis = cluster_analyzer.analyze_cluster_features(
            nasa_data=nasa_df,
            labels=clustering_result.labels,
            embeddings=embeddings
        )
        
        # ç‰¹å¾´åˆ†æå¯è¦–åŒ–ä½œæˆ
        viz_files = cluster_analyzer.create_feature_visualizations(
            analysis_results=feature_analysis,
            save_dir=self.output_dir
        )
        
        logger.info(f"âœ… Generated {len(viz_files)} cluster feature analysis visualizations")
        for viz_file in viz_files:
            logger.info(f"  ğŸ“ˆ Feature viz: {viz_file}")
        
        return feature_analysis
    
    def _save_results(self, clustering_result, texts, feature_analysis=None):
        """æœ€çµ‚çµæœä¿å­˜"""
        # åˆ†æçµæœä¿å­˜
        results = {
            "analysis_info": {
                "region": "Asia-Pacific",
                "area_coverage": {
                    "south": self.config['nasa_firms']['area_params']['south'],
                    "north": self.config['nasa_firms']['area_params']['north'],
                    "west": self.config['nasa_firms']['area_params']['west'],
                    "east": self.config['nasa_firms']['area_params']['east']
                },
                "timestamp": datetime.now().isoformat(),
                "total_samples": len(texts)
            },
            "clustering_result": clustering_result,
            "cluster_feature_analysis": feature_analysis if feature_analysis else {},
            "processing_times": self.step_times
        }
        
        result_path = os.path.join(self.output_dir, "final_asia_pacific_results.json")
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"Final results saved: {result_path}")
        
        # ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ä¿å­˜
        import pandas as pd
        import numpy as np
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’çµ„ã¿åˆã‚ã›
        labeled_data = []
        for i, (text, label) in enumerate(zip(texts, clustering_result.labels)):
            labeled_data.append({
                'id': i,
                'text': text,
                'cluster': int(label),
                'cluster_size': np.sum(clustering_result.labels == label)
            })
        
        labeled_df = pd.DataFrame(labeled_data)
        labeled_path = os.path.join(self.output_dir, "asia_pacific_fires_clustered.csv")
        labeled_df.to_csv(labeled_path, index=False, encoding='utf-8')
        logger.info(f"Labeled data saved: {labeled_path}")
        
        return result_path
    
    def _generate_comprehensive_report(self, report_generator: FireAnalysisReportGenerator, 
                                     clustering_result, feature_analysis, nasa_data):
        """åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        nasa_data_path = os.path.join(self.output_dir, "nasa_firms_data.csv")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = report_generator.generate_comprehensive_report(
            clustering_result=clustering_result,
            feature_analysis=feature_analysis,
            nasa_data_path=nasa_data_path,
            config=self.config
        )
        
        logger.info(f"ğŸ“ Comprehensive analysis report generated: {report_path}")
        return report_path
    
    def _print_summary(self, clustering_result, texts, total_time):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        labels = clustering_result.labels
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        noise_ratio = (labels == -1).sum() / len(labels) * 100 if len(labels) > 0 else 0
        
        print("\n" + "="*70)
        print("ğŸŒ ASIA-PACIFIC FOREST FIRE ANALYSIS RESULTS")
        print("="*70)
        print(f"âœ… Status: SUCCESS")
        print(f"ğŸ¯ Selected Method: {clustering_result.method}")
        print(f"ğŸ“Š Quality Score: {clustering_result.metrics.quality_score:.3f}")
        print(f"ğŸ”¢ Clusters Found: {n_clusters}")
        print(f"ğŸ“‰ Noise Ratio: {noise_ratio:.1f}%")
        print(f"ğŸ“¦ Total Fire Detections: {len(texts)}")
        print(f"â±ï¸ Processing Time: {total_time:.2f}s")
        print(f"ğŸ“ Results Directory: {self.output_dir}")
        print(f"ğŸŒ Region Coverage: Asia-Pacific (10Â°N-50Â°N, 100Â°E-180Â°E)")
        print("="*70)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ã‚¢ã‚¸ã‚¢å¤ªå¹³æ´‹åœ°åŸŸåˆ†æå®Ÿè¡Œ
        analyzer = AsiaPacificFirmsAnalyzer()
        result_path = analyzer.run_pipeline()
        
        if result_path:
            print(f"\nğŸ‰ Analysis completed successfully!")
            print(f"ğŸ“ Results saved in: {analyzer.output_dir}")
            print(f"ğŸ“„ Main results file: {result_path}")
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
            import os
            output_files = os.listdir(analyzer.output_dir)
            
            print(f"\nğŸ“Š Generated files ({len(output_files)} total):")
            for file in sorted(output_files):
                if file.endswith('.png'):
                    print(f"  ğŸ–¼ï¸  {file}")
                elif file.endswith('.json'):
                    print(f"  ğŸ“‹ {file}")
                elif file.endswith('.csv'):
                    print(f"  ğŸ“Š {file}")
                elif file.endswith('.md'):
                    print(f"  ğŸ“ {file} (ğŸ“– COMPREHENSIVE ANALYSIS REPORT)")
                else:
                    print(f"  ğŸ“„ {file}")
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹åˆ¥æ¡ˆå†…
            report_file = os.path.join(analyzer.output_dir, "comprehensive_fire_analysis_report.md")
            if os.path.exists(report_file):
                print(f"\nğŸ“– **åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ**")
                print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {report_file}")
                print(f"   å†…å®¹: 6ã¤ã®å›³è¡¨ã‚’ç”¨ã„ãŸè©³ç´°ãªç«ç½åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
                print(f"   å½¢å¼: Markdownå½¢å¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§é–²è¦§å¯èƒ½ï¼‰")
        else:
            print("âŒ Analysis failed - no data collected")
            
    except Exception as e:
        logger.error(f"Pipeline execution failed: {e}")
        raise


if __name__ == "__main__":
    main()